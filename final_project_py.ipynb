{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.py",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1zpVv1DSzX710ntw7hKWhBKbRRWeldYwH",
      "authorship_tag": "ABX9TyMtlDtw7bK4jENFMOy2rt0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puthran28/Appointment/blob/master/final_project_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8utZiJtIHxV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbfd9f42-cbd9-4371-e546-0f638b74e3ac"
      },
      "source": [
        "cd /content/drive/My Drive/srgan"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/srgan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APk5NJorIyW-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3f30a970-843b-4cf7-de3f-feb3f99d4e45"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FYAUsLBEx9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ddcec553-a5a7-4fa6-a2e6-ae01335cde66"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CTLDEgYX5KO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff9429b2-ee88-4c39-ceb0-0ccfd13a3028"
      },
      "source": [
        "pip install pillow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (7.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oqwNWTvWOqd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "48769e9b-554b-4662-b3c7-2d09f4c18d1c"
      },
      "source": [
        "pip install scipy==1.2.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.2.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/69/20c8f3b7efe362093dff891239551ff90d4c463b5f52676e2694fea09442/scipy-1.2.2-cp36-cp36m-manylinux1_x86_64.whl (24.8MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8MB 126kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.2.2) (1.18.2)\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "Successfully installed scipy-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uiVrPnocc2F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "4bf8c475-b19a-4052-82c4-c4938c94788e"
      },
      "source": [
        "pip install tensorlayer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorlayer in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied: h5py>=2.9 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.10.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.23.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.12.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.22.2.post1)\n",
            "Requirement already satisfied: cloudpickle>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (2.9.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (0.16.2)\n",
            "Requirement already satisfied: progressbar2>=3.39.3 in /usr/local/lib/python3.6/dist-packages (from tensorlayer) (3.53.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.9->tensorlayer) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21.0->tensorlayer) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21.0->tensorlayer) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21.0->tensorlayer) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21.0->tensorlayer) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->tensorlayer) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio>=2.5.0->tensorlayer) (7.0.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->tensorlayer) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->tensorlayer) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->tensorlayer) (3.2.2)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2>=3.39.3->tensorlayer) (2.4.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.15.0->tensorlayer) (4.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhSw0OamZO5Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "1487086c-733a-431a-e051-f08c679cc790"
      },
      "source": [
        "pip install scipy==1.1.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scipy==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/0b/f163da98d3a01b3e0ef1cab8dd2123c34aee2bafbb1c5bffa354cc8a1730/scipy-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.2MB)\n",
            "\u001b[K     |████████████████████████████████| 31.2MB 99kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scipy==1.1.0) (1.18.2)\n",
            "\u001b[31mERROR: tensorlayer 2.2.1 has requirement scipy>=1.2.1, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scipy\n",
            "Successfully installed scipy-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K01iLGFGV-Gv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "194d4ca2-7d97-4def-c466-60efbbe694ef"
      },
      "source": [
        "pip uninstall scipy\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling scipy-1.4.1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/scipy-1.4.1.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/scipy/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled scipy-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7FhwctPdRmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "070ce7ba-470c-4c20-dd3d-71bf87187cfd"
      },
      "source": [
        "pip install --upgrade tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbww2E-fuILq"
      },
      "source": [
        "from easydict import EasyDict as edict\n",
        "import json\n",
        "\n",
        "config = edict()\n",
        "config.TRAIN = edict()\n",
        "\n",
        "## Adam\n",
        "config.TRAIN.batch_size = 8 # [16] use 8 if your GPU memory is small, and use [2, 4] in tl.vis.save_images / use 16 for faster training\n",
        "config.TRAIN.lr_init = 1e-4\n",
        "config.TRAIN.beta1 = 0.9\n",
        "\n",
        "## initialize G\n",
        "config.TRAIN.n_epoch_init = 100\n",
        "    # config.TRAIN.lr_decay_init = 0.1\n",
        "    # config.TRAIN.decay_every_init = int(config.TRAIN.n_epoch_init / 2)\n",
        "\n",
        "## adversarial learning (SRGAN)\n",
        "config.TRAIN.n_epoch = 2000\n",
        "config.TRAIN.lr_decay = 0.1\n",
        "config.TRAIN.decay_every = int(config.TRAIN.n_epoch / 2)\n",
        "\n",
        "## train set location\n",
        "config.TRAIN.hr_img_path = '/content/drive/My Drive/srgan/DIV2K/DIV2K_train_HR'\n",
        "config.TRAIN.lr_img_path = '/content/drive/My Drive/srgan/DIV2K/DIV2K_train_LR_bicubic/X4'\n",
        "\n",
        "config.VALID = edict()\n",
        "## test set location\n",
        "config.VALID.hr_img_path = '/content/drive/My Drive/srgan/DIV2K/DIV2K_valid_HR'\n",
        "config.VALID.lr_img_path = '/content/drive/My Drive/srgan/DIV2K/DIV2K_valid_LR_bicubic/X4'\n",
        "\n",
        "def log_config(filename, cfg):\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(\"================================================\\n\")\n",
        "        f.write(json.dumps(cfg, indent=4))\n",
        "        f.write(\"\\n================================================\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtHVu4JobsW0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b265418-6308-40b5-ed56-3dd7fdcfe003"
      },
      "source": [
        "#! /usr/bin/python\n",
        "# -*- coding: utf8 -*-\n",
        " \n",
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy, multiprocessing\n",
        "import skimage.transform\n",
        "from skimage import img_as_ubyte\n",
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "from model import get_G, get_D\n",
        "from config import config\n",
        " \n",
        " \n",
        "f = open(\"/content/drive/My Drive/srgan/print.txt\", \"r\")\n",
        "print(f.read())\n",
        "path=\"/home\"\n",
        "print(os.path.join(path,\"User/Desktop\",\"file.txt\"))\n",
        "###====================== HYPER-PARAMETERS ===========================###\n",
        "## Adam\n",
        "batch_size = config.TRAIN.batch_size  # use 8 if your GPU memory is small, and change [4, 4] in tl.vis.save_images to [2, 4]\n",
        "lr_init = config.TRAIN.lr_init\n",
        "beta1 = config.TRAIN.beta1\n",
        "## initialize G\n",
        "n_epoch_init = config.TRAIN.n_epoch_init\n",
        "## adversarial learning (SRGAN)\n",
        "n_epoch = config.TRAIN.n_epoch\n",
        "lr_decay = config.TRAIN.lr_decay\n",
        "decay_every = config.TRAIN.decay_every\n",
        "shuffle_buffer_size = 128\n",
        " \n",
        "# ni = int(np.sqrt(batch_size))\n",
        " \n",
        "# create folders to save result images and trained models\n",
        "save_dir = \"/content/drive/My Drive/srgan/samples\"\n",
        "tl.files.exists_or_mkdir(save_dir)\n",
        "checkpoint_dir = \"models\"\n",
        "tl.files.exists_or_mkdir(checkpoint_dir)\n",
        " \n",
        "def get_train_data():\n",
        "    # load dataset\n",
        "    train_hr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False))#[0:20]\n",
        "        # train_lr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.lr_img_path, regx='.*.png', printable=False))\n",
        "        # valid_hr_img_list = sorted(tl.files.load_file_list(path=config.VALID.hr_img_path, regx='.*.png', printable=False))\n",
        "        # valid_lr_img_list = sorted(tl.files.load_file_list(path=config.VALID.lr_img_path, regx='.*.png', printable=False))\n",
        " \n",
        "    ## If your machine have enough memory, please pre-load the entire train set.\n",
        "    train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=config.TRAIN.hr_img_path, n_threads=32)\n",
        "        # for im in train_hr_imgs:\n",
        "        #     print(im.shape)\n",
        "        # valid_lr_imgs = tl.vis.read_images(valid_lr_img_list, path=config.VALID.lr_img_path, n_threads=32)\n",
        "        # for im in valid_lr_imgs:\n",
        "        #     print(im.shape)\n",
        "        # valid_hr_imgs = tl.vis.read_images(valid_hr_img_list, path=config.VALID.hr_img_path, n_threads=32)\n",
        "        # for im in valid_hr_imgs:\n",
        "        #     print(im.shape)\n",
        "        \n",
        "    # dataset API and augmentation\n",
        "    def generator_train():\n",
        "        for img in train_hr_imgs:\n",
        "            yield img\n",
        "    def _map_fn_train(img):\n",
        "        hr_patch = tf.image.random_crop(img, [384, 384, 3])\n",
        "        hr_patch = hr_patch / (255. / 2.)\n",
        "        hr_patch = hr_patch - 1.\n",
        "        hr_patch = tf.image.random_flip_left_right(hr_patch)\n",
        "        lr_patch = tf.image.resize(hr_patch, size=[96, 96])\n",
        "        return lr_patch, hr_patch\n",
        "    train_ds = tf.data.Dataset.from_generator(generator_train, output_types=(tf.float32))\n",
        "    train_ds = train_ds.map(_map_fn_train, num_parallel_calls=multiprocessing.cpu_count())\n",
        "        # train_ds = train_ds.repeat(n_epoch_init + n_epoch)\n",
        "    train_ds = train_ds.shuffle(shuffle_buffer_size)\n",
        "    train_ds = train_ds.prefetch(buffer_size=2)\n",
        "    train_ds = train_ds.batch(batch_size)\n",
        "        # value = train_ds.make_one_shot_iterator().get_next()\n",
        "    return train_ds\n",
        " \n",
        "def train():\n",
        "    G = get_G((batch_size, 96, 96, 3))\n",
        "    D = get_D((batch_size, 384, 384, 3))\n",
        "    VGG = tl.models.vgg19(pretrained=True, end_with='pool4', mode='static')\n",
        " \n",
        "    lr_v = tf.Variable(lr_init)\n",
        "    g_optimizer_init = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
        "    g_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
        "    d_optimizer = tf.optimizers.Adam(lr_v, beta_1=beta1)\n",
        " \n",
        "    G.train()\n",
        "    D.train()\n",
        "    VGG.train()\n",
        " \n",
        "    train_ds = get_train_data()\n",
        " \n",
        "    ## initialize learning (G)\n",
        "    n_step_epoch = round(n_epoch_init // batch_size)\n",
        "    for epoch in range(n_epoch_init):\n",
        "        for step, (lr_patchs, hr_patchs) in enumerate(train_ds):\n",
        "            if lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
        "                break\n",
        "            step_time = time.time()\n",
        "            with tf.GradientTape() as tape:\n",
        "                fake_hr_patchs = G(lr_patchs)\n",
        "                mse_loss = tl.cost.mean_squared_error(fake_hr_patchs, hr_patchs, is_mean=True)\n",
        "            grad = tape.gradient(mse_loss, G.trainable_weights)\n",
        "            g_optimizer_init.apply_gradients(zip(grad, G.trainable_weights))\n",
        "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.3f} \".format(\n",
        "                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss))\n",
        "        if (epoch != 0) and (epoch % 10 == 0):\n",
        "            tl.vis.save_images(fake_hr_patchs.numpy(), [2, 4], os.path.join(save_dir, 'train_g_init_{}.png'.format(epoch)))\n",
        " \n",
        "    ## adversarial learning (G, D)\n",
        "    n_step_epoch = round(n_epoch // batch_size)\n",
        "    for epoch in range(n_epoch):\n",
        "        for step, (lr_patchs, hr_patchs) in enumerate(train_ds):\n",
        "            if lr_patchs.shape[0] != batch_size: # if the remaining data in this epoch < batch_size\n",
        "                break\n",
        "            step_time = time.time()\n",
        "            with tf.GradientTape(persistent=True) as tape:\n",
        "                fake_patchs = G(lr_patchs)\n",
        "                logits_fake = D(fake_patchs)\n",
        "                logits_real = D(hr_patchs)\n",
        "                feature_fake = VGG((fake_patchs+1)/2.) # the pre-trained VGG uses the input range of [0, 1]\n",
        "                feature_real = VGG((hr_patchs+1)/2.)\n",
        "                d_loss1 = tl.cost.sigmoid_cross_entropy(logits_real, tf.ones_like(logits_real))\n",
        "                d_loss2 = tl.cost.sigmoid_cross_entropy(logits_fake, tf.zeros_like(logits_fake))\n",
        "                d_loss = d_loss1 + d_loss2\n",
        "                g_gan_loss = 1e-3 * tl.cost.sigmoid_cross_entropy(logits_fake, tf.ones_like(logits_fake))\n",
        "                mse_loss = tl.cost.mean_squared_error(fake_patchs, hr_patchs, is_mean=True)\n",
        "                vgg_loss = 2e-6 * tl.cost.mean_squared_error(feature_fake, feature_real, is_mean=True)\n",
        "                g_loss = mse_loss + vgg_loss + g_gan_loss\n",
        "            grad = tape.gradient(g_loss, G.trainable_weights)\n",
        "            g_optimizer.apply_gradients(zip(grad, G.trainable_weights))\n",
        "            grad = tape.gradient(d_loss, D.trainable_weights)\n",
        "            d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\n",
        "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss(mse:{:.3f}, vgg:{:.3f}, adv:{:.3f}) d_loss: {:.3f}\".format(\n",
        "                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, mse_loss, vgg_loss, g_gan_loss, d_loss))\n",
        " \n",
        "        # update the learning rate\n",
        "        if epoch != 0 and (epoch % decay_every == 0):\n",
        "            new_lr_decay = lr_decay**(epoch // decay_every)\n",
        "            lr_v.assign(lr_init * new_lr_decay)\n",
        "            log = \" ** new learning rate: %f (for GAN)\" % (lr_init * new_lr_decay)\n",
        "            print(log)\n",
        " \n",
        "        if (epoch != 100): #and (epoch % 10 == 0):\n",
        "            tl.vis.save_images(fake_patchs.numpy(), [2, 4], os.path.join(save_dir, 'train_g_{}.png'.format(epoch)))\n",
        "            G.save_weights(\"/content/drive/My Drive/srgan/models/g.h5\")  #os.path.join(checkpoint_dir, 'g.h5')\n",
        "            D.save_weights(\"/content/drive/My Drive/srgan/models/d.h5\")  #os.path.join(checkpoint_dir, 'd.h5')\n",
        " \n",
        "def evaluate():\n",
        "    ###====================== PRE-LOAD DATA ===========================###\n",
        "    # train_hr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.hr_img_path, regx='.*.png', printable=False))\n",
        "    # train_lr_img_list = sorted(tl.files.load_file_list(path=config.TRAIN.lr_img_path, regx='.*.png', printable=False))\n",
        "    valid_hr_img_list = sorted(tl.files.load_file_list(path=config.VALID.hr_img_path, regx='.*.png', printable=False))\n",
        "    valid_lr_img_list = sorted(tl.files.load_file_list(path=config.VALID.lr_img_path, regx='.*.png', printable=False))\n",
        " \n",
        "    ## if your machine have enough memory, please pre-load the whole train set.\n",
        "    # train_hr_imgs = tl.vis.read_images(train_hr_img_list, path=config.TRAIN.hr_img_path, n_threads=32)\n",
        "    # for im in train_hr_imgs:\n",
        "    #     print(im.shape)\n",
        "    valid_lr_imgs = tl.vis.read_images(valid_lr_img_list, path=config.VALID.lr_img_path, n_threads=32)\n",
        "    # for im in valid_lr_imgs:\n",
        "    #     print(im.shape)\n",
        "    valid_hr_imgs = tl.vis.read_images(valid_hr_img_list, path=config.VALID.hr_img_path, n_threads=32)\n",
        "    # for im in valid_hr_imgs:\n",
        "    #     print(im.shape)\n",
        " \n",
        "    ###========================== DEFINE MODEL ============================###\n",
        "    imid = 6  \n",
        "    valid_lr_img = valid_lr_imgs[imid]\n",
        "    valid_hr_img = valid_hr_imgs[imid]\n",
        "    # valid_lr_img = get_imgs_fn()\n",
        "    valid_lr_img = (valid_lr_img / 127.5) - 1  # rescale to ［－1, 1]\n",
        "    # print(valid_lr_img.min(), valid_lr_img.max())\n",
        " \n",
        "    G = get_G([1, None, None, 3])\n",
        "    G.load_weights(os.path.join(checkpoint_dir, 'g.h5'))\n",
        "    G.eval()\n",
        " \n",
        "    valid_lr_img = np.asarray(valid_lr_img, dtype=np.float32)\n",
        "    valid_lr_img = valid_lr_img[np.newaxis,:,:,:]\n",
        "    size = [valid_lr_img.shape[1], valid_lr_img.shape[2]]\n",
        " \n",
        "    out = G(valid_lr_img).numpy()\n",
        " \n",
        "    print(\"LR size: %s /  generated HR size: %s\" % (size, out.shape))  # LR size: (339, 510, 3) /  gen HR size: (1, 1356, 2040, 3)\n",
        "    print(\"[*] save images\")\n",
        "    tl.vis.save_image(out[0], os.path.join(save_dir, 'valid_gen.png'))\n",
        "    tl.vis.save_image(valid_lr_img[0], os.path.join(save_dir, 'valid_lr.png'))\n",
        "    tl.vis.save_image(valid_hr_img, os.path.join(save_dir, 'valid_hr.png'))\n",
        " \n",
        "    out_bicu = skimage.transform.resize(valid_lr_img[0], [size[0] * 4, size[1] * 4])\n",
        "    tl.vis.save_image(out_bicu, os.path.join(save_dir, 'valid_bicubic.png'))\n",
        " \n",
        " \n",
        "if __name__ == '__main__':   \n",
        "    evaluate()\n",
        "    #train()\n",
        "    # import argparse\n",
        "    # parser = argparse.ArgumentParser()\n",
        " \n",
        "    # parser.add_argument('--mode', type=str, default='srgan', help='srgan, evaluate')\n",
        " \n",
        "    # args = parser.parse_args()\n",
        " \n",
        "   # tl.global_flag['mode'] = '--mode' #args.mode\n",
        " \n",
        "    # if tl.global_flag['mode'] == 'srgan':\n",
        "    #     train()\n",
        "    # elif tl.global_flag['mode'] == 'evaluate':\n",
        "    #     evaluate()\n",
        "    # else:\n",
        "    #     raise Exception(\"Unknow --mode\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "heloo mangesh\n",
            "\n",
            "/home/User/Desktop/file.txt\n",
            "[TL] [!] /content/drive/My Drive/srgan/samples exists ...\n",
            "[TL] [!] models exists ...\n",
            "[TL] read 10 from DIV2K/DIV2K_valid_LR_bicubic/X4/\n",
            "[TL] read 10 from DIV2K/DIV2K_valid_HR/\n",
            "[TL] Input  _inputlayer_2: [1, None, None, 3]\n",
            "[TL] Conv2d conv2d_38: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
            "[TL] Conv2d conv2d_39: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_34: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_40: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_35: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_18: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_41: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_36: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_42: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_37: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_19: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_43: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_38: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_44: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_39: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_20: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_45: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_40: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_46: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_41: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_21: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_47: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_42: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_48: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_43: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_22: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_49: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_44: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_50: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_45: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_23: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_51: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_46: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_52: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_47: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_24: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_53: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_48: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_54: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_49: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_25: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_55: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_50: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_56: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_51: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_26: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_57: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_52: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_58: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_53: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_27: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_59: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_54: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_60: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_55: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_28: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_61: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_56: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_62: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_57: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_29: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_63: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_58: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_64: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_59: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_30: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_65: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_60: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_66: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_61: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_31: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_67: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_62: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_68: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_63: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_32: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_69: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_64: decay: 0.900000 epsilon: 0.000010 act: relu is_train: False\n",
            "[TL] Conv2d conv2d_70: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_65: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_33: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_71: n_filter: 64 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] BatchNorm batchnorm2d_66: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: False\n",
            "[TL] Elementwise elementwise_34: fn: add act: No Activation\n",
            "[TL] Conv2d conv2d_72: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] SubpixelConv2d  subpixelconv2d_3: scale: 2 act: relu\n",
            "[TL] Conv2d conv2d_73: n_filter: 256 filter_size: (3, 3) strides: (1, 1) pad: SAME act: No Activation\n",
            "[TL] SubpixelConv2d  subpixelconv2d_4: scale: 2 act: relu\n",
            "[TL] Conv2d conv2d_74: n_filter: 3 filter_size: (1, 1) strides: (1, 1) pad: SAME act: tanh\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1104dd8ee631>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;31m#train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;31m# import argparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-1104dd8ee631>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# print(valid_lr_img.min(), valid_lr_img.max())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/My Drive/srgan/model.py\u001b[0m in \u001b[0;36mget_G\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"generator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorlayer/models/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_global_model_name_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0;34m'Model name \\'%s\\' has already been used by another model. Please change the model name.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m                 )\n\u001b[1;32m    175\u001b[0m             \u001b[0m_global_model_name_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Model name 'generator' has already been used by another model. Please change the model name."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcLHu3fCZJWw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f2f20887-11ef-454c-f465-23ef513b4a8d"
      },
      "source": [
        "pip install --upgrade numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}